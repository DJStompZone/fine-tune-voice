{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "private_outputs": true,
            "provenance": [
            ],
            "machine_shape": "hm",
            "gpuType": "A100"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "## Training Script for Fine-Tuning TTS Model\n",
                "> ### What could *possibly* go wrong?\n",
                "\n",
                "---\n",
                "Copyright\n",
                "---------\n",
                "DJ Stomp 2025\n",
                "\n",
                "License\n",
                "-------\n",
                "* Code for training and auto annotation:\n",
                "    MIT\n",
                "    See the LICENSE file for full details.\n",
                "\n",
                "* Weights, fine-tunings, artifacts, etc:\n",
                "    CC-BY-NC-SA 4.0\n",
                "    Insofar as is required by the terms stipulated in the hereditary Creative Commons license.\n",
                "    See the LICENSE file for full details.\n",
                "\n",
                "* Note:\n",
                "    For the purposes of disambiguation, any textual material in this project shall be considered to be licensed as MIT unless specifically declared otherwise."
            ],
            "metadata": {
                "id": "intro-markdown"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Install Dependencies\n",
                "!pip install torch torchvision torchaudio transformers datasets huggingface_hub soundfile tqdm requests python-dotenv\n",
                "!pip install ffmpeg-python\n",
                "!apt-get install -y sox ffmpeg\n",
                "\n",
                "# Clone repository\n",
                "!git clone https://github.com/fishaudio/fish-speech.git\n",
                "%cd fish-speech\n",
                "!pip install -e .[stable]"
            ],
            "metadata": {
                "id": "install-dependencies"
            },
            "execution_count": null,
            "outputs": [
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# Imports\n",
                "import os\n",
                "import time\n",
                "import requests\n",
                "from dotenv import load_dotenv\n",
                "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n",
                "from huggingface_hub import HfFolder\n",
                "from datasets import load_dataset\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Load environment variables\n",
                "load_dotenv()"
            ],
            "metadata": {
                "id": "imports-and-env"
            },
            "execution_count": null,
            "outputs": [
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# Configuration\n",
                "DATASET_PATH = os.getenv(\"DATASET_PATH\")\n",
                "OUTPUT_DIR = os.getenv(\"OUTPUT_DIR\")\n",
                "HF_REPO_NAME = os.getenv(\"HF_REPO_NAME\")\n",
                "DISCORD_WEBHOOK_URL = os.getenv(\"DISCORD_WEBHOOK_URL\")\n",
                "HUGGINGFACE_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
                "BATCH_SIZE = int(os.getenv(\"BATCH_SIZE\", 16))\n",
                "NUM_EPOCHS = int(os.getenv(\"NUM_EPOCHS\", 20))\n",
                "SAVE_STEPS = int(os.getenv(\"SAVE_STEPS\", 500))\n",
                "SAVE_TOTAL_LIMIT = int(os.getenv(\"SAVE_TOTAL_LIMIT\", 3))\n",
                "LOGGING_STEPS = int(os.getenv(\"LOGGING_STEPS\", 100))\n",
                "CHECKPOINT_DIR = os.getenv(\"CHECKPOINT_DIR\")\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "os.makedirs(CHECKPOINT_DIR, exist_ok=True)"
            ],
            "metadata": {
                "id": "configurations"
            },
            "execution_count": null,
            "outputs": [
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# Discord Notification Function\n",
                "def send_discord_message(message):\n",
                "    \"\"\"Sends a message to Discord using the provided webhook URL.\"\"\"\n",
                "    payload = {\"content\": f\"[**StompNET**] {message}\"}\"\n",
                "    try:\n",
                "        response = requests.post(DISCORD_WEBHOOK_URL, json=payload)\n",
                "        response.raise_for_status()\n",
                "        print(f\"Discord message sent: {message}\")\n",
                "    except requests.exceptions.RequestException as e:\n",
                "        print(f\"Failed to send Discord message: {e}\")"
            ],
            "metadata": {
                "id": "discord-function"
            },
            "execution_count": null,
            "outputs": [
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# Validate Dataset\n",
                "def validate_dataset(dataset_path):\n",
                "    \"\"\"Validates the dataset structure and content.\"\"\"\n",
                "    audio_dir = os.path.join(dataset_path, \"audio\")\n",
                "    transcriptions_dir = os.path.join(dataset_path, \"transcriptions\")\n",
                "\n",
                "    if not os.path.exists(audio_dir):\n",
                "        raise FileNotFoundError(f\"Audio directory is missing: {audio_dir}\")\n",
                "    if not os.path.exists(transcriptions_dir):\n",
                "        raise FileNotFoundError(f\"Transcriptions directory is missing: {transcriptions_dir}\")\n",
                "\n",
                "    audio_files = os.listdir(audio_dir)\n",
                "    transcription_files = os.listdir(transcriptions_dir)\n",
                "\n",
                "    missing_transcriptions = []\n",
                "    for audio_file in audio_files:\n",
                "        base_name = os.path.splitext(audio_file)[0]\n",
                "        transcription_file = f\"{base_name}.txt\"\n",
                "        if transcription_file not in transcription_files:\n",
                "            missing_transcriptions.append(audio_file)\n",
                "\n",
                "    if missing_transcriptions:\n",
                "        raise ValueError(\n",
                "            f\"The following audio files are missing transcriptions: {', '.join(missing_transcriptions)}\"\n",
                "        )\n",
                "\n",
                "    print(f\"Dataset validation complete: {len(audio_files)} audio files and transcriptions found.\")\n",
                "    return len(audio_files)"
            ],
            "metadata": {
                "id": "validate-dataset"
            },
            "execution_count": null,
            "outputs": [
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# Validate Dataset\n",
                "try:\n",
                "    send_discord_message(\"üîç Validating dataset...\")\n",
                "    num_validated = validate_dataset(DATASET_PATH)\n",
                "    send_discord_message(f\"‚úÖ Dataset validation complete: {num_validated} training records validated.\")\n",
                "except Exception as e:\n",
                "    send_discord_message(f\"‚ùå Dataset validation failed: {str(e)}\")\n",
                "    raise"
            ],
            "metadata": {
                "id": "validate-dataset-call"
            },
            "execution_count": null,
            "outputs": [
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# Initialize Model and Tokenizer\n",
                "try:\n",
                "    audio_model = \"fishaudio/fish-speech-1.5\"\n",
                "    send_discord_message(f'üß∞ Initializing model and tokenizer: \"{audio_model}\"...')\n",
                "    tokenizer = AutoTokenizer.from_pretrained(audio_model)\n",
                "    model = AutoModelForCausalLM.from_pretrained(audio_model)\n",
                "    send_discord_message(\"‚úÖ Model and tokenizer initialized successfully.\")\n",
                "except Exception as e:\n",
                "    send_discord_message(f\"‚ùå Model initialization failed: {str(e)}\")\n",
                "    raise"
            ],
            "metadata": {
                "id": "initialize-model"
            },
            "execution_count": null,
            "outputs": [
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# Training Arguments\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=OUTPUT_DIR,\n",
                "    per_device_train_batch_size=BATCH_SIZE,\n",
                "    num_train_epochs=NUM_EPOCHS,\n",
                "    save_steps=SAVE_STEPS,\n",
                "    save_total_limit=SAVE_TOTAL_LIMIT,\n",
                "    logging_dir=\"./logs\",\n",
                "    logging_steps=LOGGING_STEPS,\n",
                "    report_to=\"hub\",\n",
                "    hub_model_id=HF_REPO_NAME,\n",
                "    hub_token=HUGGINGFACE_TOKEN,\n",
                "    push_to_hub=True,\n",
                "    resume_from_checkpoint=True\n",
                ")"
            ],
            "metadata": {
                "id": "training-arguments"
            },
            "execution_count": null,
            "outputs": [
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# Load and Tokenize Dataset\n",
                "try:\n",
                "    send_discord_message(f\"üß± Loading dataset from {DATASET_PATH}/train.json...\")\n",
                "    train_dataset = load_dataset(\"json\", data_files={\"train\": f\"{DATASET_PATH}/train.json\"})[\"train\"]\n",
                "\n",
                "    def tokenize_function(examples):\n",
                "        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
                "\n",
                "    tokenized_dataset = train_dataset.map(tokenize_function, batched=True)\n",
                "    send_discord_message(\"‚úÖ Dataset loaded and tokenized successfully.\")\n",
                "except Exception as e:\n",
                "    send_discord_message(f\"‚ùå Dataset loading or tokenization failed: {str(e)}\")\n",
                "    raise"
            ],
            "metadata": {
                "id": "load-and-tokenize"
            },
            "execution_count": null,
            "outputs": [
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# Trainer Setup\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=tokenized_dataset,\n",
                "    tokenizer=tokenizer\n",
                ")"
            ],
            "metadata": {
                "id": "trainer-setup"
            },
            "execution_count": null,
            "outputs": [
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# Start Training\n",
                "start_time = elapsed_time = time.time()\n",
                "try:\n",
                "    send_discord_message(\"üéØ Starting fine-tuning...\")\n",
                "    send_discord_message(\"\n\".join([\n",
                "        f\"Initiating training session:\", \n",
                "        f\"  {NUM_EPOCHS=}\",\n",
                "        f\"  {SAVE_STEPS=}\",\n",
                "        f\"  {OUTPUT_DIR=}\",\n",
                "        f\"  {BATCH_SIZE=}\"\n",
                "    ]))\n",
                "    start_time = time.time()\n",
                "    trainer.train()\n",
                "    elapsed_time = time.time() - start_time\n",
                "    elapsed_minutes = elapsed_time / 60\n",
                "    send_discord_message(f\"‚úÖ Fine-tuning completed successfully in {elapsed_minutes:.2f} minutes.\")\n",
                "except Exception as e:\n",
                "    elapsed_time = time.time() - start_time\n",
                "    elapsed_minutes = elapsed_time / 60\n",
                "    send_discord_message(f\"‚ùå Fine-tuning failed after {elapsed_minutes:.2f} minutes: {str(e)}\")\n",
                "    raise"
            ],
            "metadata": {
                "id": "start-training"
            },
            "execution_count": null,
            "outputs": [
            ]
        },
        {
            "cell_type": "code",
            "source": [
                "# Push Final Model\n",
                "try:\n",
                "    send_discord_message(\"üöÄ Pushing final model to Hugging Face Hub...\")\n",
                "    trainer.push_to_hub(commit_message=\"Fine-tuned Fish-Speech model.\")\n",
                "    send_discord_message(\"‚úÖ Final model pushed to Hugging Face Hub.\")\n",
                "except Exception as e:\n",
                "    send_discord_message(f\"‚ùå Failed to push model to Hugging Face Hub: {str(e)}\")\n",
                "    raise"
            ],
            "metadata": {
                "id": "push-final-model"
            },
            "execution_count": null,
            "outputs": [
            ]
        }
    ]
}